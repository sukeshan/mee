<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Why We Pad | Sukesh üåñ</title>
    <link rel="stylesheet" href="../style.css">
</head>

<body>
    <div class="container">
        <nav>
            <div class="logo"><a href="../index.html">Sukesh</a></div>
            <div class="nav-links">
                <a href="../blog.html">Back to Blog</a>
            </div>
        </nav>

        <article>
            <div class="article-header">
                <span class="blog-meta">NLP / Optimization</span>
                <h1>Why We Pad + Dynamic Padding</h1>
            </div>

            <div class="article-content">
                <h3>Why We Pad</h3>
                <p>GPUs work much better when they can process several training samples at the same time, so we feed
                    them a ‚Äúbatch‚Äù of samples. However, for a GPU to handle a batch, every input must be the same
                    length. To make this work, we add padding to all our input sequences to reach a fixed length. We
                    also give the model an ‚Äúattention mask‚Äù for each sample. This mask marks the [PAD] tokens and tells
                    BERT to ignore them.</p>

                <h3>Dynamic Padding</h3>
                <p>Even though the attention mask makes sure that the [PAD] tokens don‚Äôt affect how BERT reads the text,
                    these tokens still go through all the math operations in BERT. This can slow down training and
                    evaluation a bit. And while every sample in a batch must be the same length, BERT doesn‚Äôt mind what
                    that length is‚Äîwe can use different maximum lengths for different batches!</p>

                <h3>Uniform Length Batching</h3>
                <p>We can go one step further by sorting our dataset by length before creating the batches, making the
                    process even more efficient. However, there is a caveat. In each epoch, the dataloader returns
                    batches in the same order. This consistency may lead the model to memorize the pattern, increasing
                    the risk of overfitting. To avoid this, we need to randomly select indices and create batches
                    accordingly.</p>

                <h3>Key Tokenization Metrics</h3>
                <ul>
                    <li><strong>Fertility:</strong> Refers to how many subwords or tokens a tokenizer generates when
                        processing a given text.</li>
                    <li><strong>Proportion of Continued Words:</strong> Subwords that are parts of a word but not the
                        start of a word.</li>
                    <li><strong>Normalized Sequence Length (NSL):</strong> Evaluates the compression efficiency of a
                        tokenizer.</li>
                </ul>
            </div>
        </article>

        <footer>
            <div>
                <p>¬© 2025 Sukesh</p>
            </div>
        </footer>
    </div>
</body>

</html>